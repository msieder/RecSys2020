{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Create Spark Context\n",
    "sc = SparkContext()\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Save path in variable\n",
    "path = 'hdfs:///user/pknees/RSC20/training.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the file\n",
    "df = spark.read \\\n",
    "          .load(path,\n",
    "           format=\"csv\",delimiter=\"\\x01\")\n",
    "\n",
    "# Changing Column names\n",
    "df = df.toDF(\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\\\n",
    "               \"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text_tokens='101\\t1942\\t18628\\t15752\\t4458\\t7697\\t24309\\t10634\\t5618\\t2395\\t2598\\t3584\\t1946\\t22480\\t67316\\t12434\\t19054\\t10634\\t4704\\t3350\\t1910\\t15752\\t106975\\t15355\\t10083\\t129\\t2583\\t2042\\t29004\\t58268\\t111806\\t18628\\t102', hashtags=None, tweet_id='E7D6C5094767223F6F8789A87A1937AB', present_media=None, present_links=None, present_domains=None, tweet_type='TopLevel', language='22C448FF81263D4BAF2A176145EE9EAD', tweet_timestamp='1581262691', engaged_with_user_id='D557B03872EF8986F7F4426AE094B2FE', engaged_with_user_follower_count='986', engaged_with_user_following_count='1201', engaged_with_user_is_verified='false', engaged_with_user_account_creation='1274269909', engaging_user_id='00000776B07587ECA9717BFC301F2D6E', engaging_user_follower_count='94', engaging_user_following_count='648', engaging_user_is_verified='false', engaging_user_account_creation='1478011810', engaged_follows_engaging='false', reply_timestamp=None, retweet_timestamp=None, retweet_with_comment_timestamp=None, like_timestamp=None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the data\n",
    "df.take(1)\n",
    "#[Row(text_tokens='101\\t1942\\t18628\\t15752\\t4458\\t7697\\t24309\\t10634\\t5618\\t2395\\t2598\\t3584\\t1946\\t22480\\t67316\\t12434\\t19054\\t10634\\t4704\\t3350\\t1910\\t15752\\t106975\\t15355\\t10083\\t129\\t2583\\t2042\\t29004\\t58268\\t111806\\t18628\\t102', \\ \n",
    "#hashtags=None, tweet_id='E7D6C5094767223F6F8789A87A1937AB', present_media=None, present_links=None, present_domains=None, tweet_type='TopLevel', language='22C448FF81263D4BAF2A176145EE9EAD', tweet_timestamp='1581262691', \\\n",
    "#engaged_with_user_id='D557B03872EF8986F7F4426AE094B2FE', engaged_with_user_follower_count='986', engaged_with_user_following_count='1201', engaged_with_user_is_verified='false', engaged_with_user_account_creation='1274269909', \\\n",
    "#engaging_user_id='00000776B07587ECA9717BFC301F2D6E', engaging_user_follower_count='94', engaging_user_following_count='648', engaging_user_is_verified='false', engaging_user_account_creation='1478011810', \\\n",
    "#engaged_follows_engaging='false', reply_timestamp=None, retweet_timestamp=None, retweet_with_comment_timestamp=None, like_timestamp=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tweets?\n",
    "#df.count()\n",
    "# 121.386.431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Column Subsets**\n",
    "\n",
    "* id_features with large vocabulary. We will first hash them and then bucketize them (Unclear if One-Hot-Encoded later)\n",
    "* numeric_features will get bucketized and One-Hot-Encoded\n",
    "* categorical_features will get StringIndexed (into numeric values) and One-Hot-Encoded\n",
    "* text_features contain information on Hashtags etc. - We will use HashingTF\n",
    "* label_columns are the dependent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_features = [\"tweet_id\",\"engaging_user_id\",\"engaged_with_user_id\"]\n",
    "\n",
    "numeric_features = [\"tweet_timestamp\",\n",
    "                    \"engaged_with_user_follower_count\", \"engaged_with_user_following_count\", \"engaged_with_user_account_creation\",\n",
    "                    \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_account_creation\"\n",
    "                   ]\n",
    "\n",
    "categorical_features = [\"tweet_type\", \"language\", \n",
    "                        \"engaged_with_user_is_verified\", \"engaging_user_is_verified\", \"engaged_follows_engaging\"\n",
    "                       ]\n",
    "\n",
    "text_features = [\"text_tokens\", \"hashtags\", \"present_media\", \"present_links\", \"present_domains\"]\n",
    "\n",
    "label_columns = [\"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "* Split Tab-Seperated text_features into arrays (special handling for \"None\")\n",
    "* Convert different pseudo-integer columns (integer-strings) to Integers\n",
    "* Hash the ID-Values and bin them in 50 buckets (using modulo)\n",
    "* Convert Text-Feature Arrays to Vectors\n",
    "* Convert the interaction columns from a timestamp to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "for feature in text_features:\n",
    "    text_feature_split = f.split(df[feature], '\\t')\n",
    "    df = df.withColumn(feature, f.when(f.col(feature).isNotNull(), text_feature_split).otherwise(f.array().cast(\"array<string>\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "for feature in numeric_features:\n",
    "    df = df.withColumn(feature,df[feature].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in id_features:\n",
    "    output_col = feature + \"_hashed\"\n",
    "    df = df.withColumn(output_col, (f.hash(f.col(feature))))\n",
    "    df = df.withColumn(output_col, f.when(f.col(output_col) < 0, f.col(output_col)*-1%50).otherwise(f.col(output_col)%50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "#from pyspark.sql.functions import udf\n",
    "\n",
    "#list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "#for feature in text_features:\n",
    "#    df = df.withColumn(feature,list_to_vector_udf(df[feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "for col in label_columns:\n",
    "    df = df.withColumn(\"label\", f.when(f.col(col).isNotNull(), a).otherwise(0))\n",
    "    a = a+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text_tokens=['101', '1942', '18628', '15752', '4458', '7697', '24309', '10634', '5618', '2395', '2598', '3584', '1946', '22480', '67316', '12434', '19054', '10634', '4704', '3350', '1910', '15752', '106975', '15355', '10083', '129', '2583', '2042', '29004', '58268', '111806', '18628', '102'], hashtags=[], tweet_id='E7D6C5094767223F6F8789A87A1937AB', present_media=[], present_links=[], present_domains=[], tweet_type='TopLevel', language='22C448FF81263D4BAF2A176145EE9EAD', tweet_timestamp=1581262691, engaged_with_user_id='D557B03872EF8986F7F4426AE094B2FE', engaged_with_user_follower_count=986, engaged_with_user_following_count=1201, engaged_with_user_is_verified='false', engaged_with_user_account_creation=1274269909, engaging_user_id='00000776B07587ECA9717BFC301F2D6E', engaging_user_follower_count=94, engaging_user_following_count=648, engaging_user_is_verified='false', engaging_user_account_creation=1478011810, engaged_follows_engaging='false', reply_timestamp=0, retweet_timestamp=0, retweet_with_comment_timestamp=0, like_timestamp=0, tweet_id_hashed=3, engaging_user_id_hashed=15, engaged_with_user_id_hashed=31)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**\n",
    "\n",
    "* featureQunatileDiscretizer for Bucketizing numeric features\n",
    "* featureStringIndexer for Converting category-strings to category-ints\n",
    "* *featureFeatureHasher for Hashing the ID features into 50 bins*  ***depreciated***\n",
    "* featureHashingTF for vectorizing the text features\n",
    "* encoder for One-Hot-Encoding of features\n",
    "* VectorAssembler for combining all the oneHot features into a single feature\n",
    "* AllRFModels to build 4 different RF Models for interaction prediction \n",
    "\n",
    "Create a Pipeline for all Stages at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the numbers of quantiles/buckets for the baseline approach\n",
    "nq = 50\n",
    "\n",
    "from pyspark.ml.feature import QuantileDiscretizer, StringIndexer, FeatureHasher, HashingTF, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "AllQuantileDiscretizers = [QuantileDiscretizer(numBuckets=nq,\n",
    "                                      inputCol=col,\n",
    "                                      outputCol=(col + \"_bucketized\"),\n",
    "                                      handleInvalid=\"keep\") for col in numeric_features]\n",
    "\n",
    "AllStringIndexers = [StringIndexer(inputCol=col, \n",
    "                            outputCol=(col + \"_indexed\")) for col in categorical_features]\n",
    "\n",
    "### FeatureHasher has been adapted to a hardcoded feature hashing + bucketing in the preprocessing step\n",
    "#AllFeatureHashers = [FeatureHasher(numFeatures=nq,\n",
    "#                           inputCols=[col],\n",
    "#                           outputCol=(col + \"_hashed\")) for col in id_features]\n",
    "\n",
    "\n",
    "AllHashingTF = [HashingTF(inputCol=col, \n",
    "                          outputCol=(col + \"_vectorized\")) for col in text_features]\n",
    "\n",
    "to_onehot_features = [col + \"_bucketized\" for col in numeric_features]\n",
    "to_onehot_features.extend(col + \"_indexed\" for col in categorical_features)\n",
    "to_onehot_features.extend(col + \"_hashed\" for col in id_features)\n",
    "\n",
    "onehot_features = [col + \"_oneHot\" for col in numeric_features]\n",
    "onehot_features.extend(col + \"_oneHot\" for col in categorical_features)\n",
    "onehot_features.extend(col + \"_oneHot\" for col in id_features)\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=to_onehot_features,\n",
    "                                 outputCols=onehot_features)\n",
    "\n",
    "assembler_features = VectorAssembler(\n",
    "    inputCols=onehot_features,\n",
    "    outputCol=\"features_oneHot\")\n",
    "\n",
    "#assembler_labels = VectorAssembler(\n",
    "#    inputCols=label_columns,\n",
    "#    outputCol=\"label\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AllRFModels = [RandomForestClassifier(labelCol=col, featuresCol=\"features_oneHot\",predictionCol=(col+\"_prediction\"), numTrees=10) for col in label_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "AllStages = list()\n",
    "AllStages.extend(AllQuantileDiscretizers)\n",
    "AllStages.extend(AllStringIndexers)\n",
    "#AllStages.extend(AllFeatureHashers) #depreciated\n",
    "AllStages.extend(AllHashingTF)\n",
    "AllStages.append(encoder)\n",
    "AllStages.append(assembler_features)\n",
    "#AllStages.append(assembler_labels) #depreciated\n",
    "#AllStages.extend(AllRFModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Note for Pipeline**\n",
    "\n",
    "For now the Random Forest Models are not included in the Pipeline. However, they are already initialized - meaning that one can just \"uncomment\" them and include them in the pipeline. Then we should be able to fit the pipleline on the data - save the resulting model and transform the test set.\n",
    "\n",
    "At this point we still have to read in the test set and apply our preprocessing (except for the label_columns) to the test set. But in the end this should just be a copy+paste job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=AllStages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**\n",
    "\n",
    "For this part we generate a train and a test set and fit our pipeline.\n",
    "\n",
    "Additionally we build + predict from a single RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, rest = df.randomSplit([0.005, 0.005, 0.99], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pipeline_model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rfModel.transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=1, llike_timestamp_prediction=1.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=1, llike_timestamp_prediction=0.0),\n",
       " Row(like_timestamp=0, llike_timestamp_prediction=0.0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.select([\"like_timestamp\",\"like_timestamp_prediction\"]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.428965\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"like_timestamp\", predictionCol=\"llike_timestamp_prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_pred)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#layers = [4, 5, 4, 4]\n",
    "#trainer = MultilayerPerceptronClassifier(featuresCol=\"features_oneHot\",labelCol=\"label\",maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "#model = trainer.fit(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
