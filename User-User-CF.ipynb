{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse.linalg import norm\n",
    "import sklearn.preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'training_sample.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\\\n",
    "               \"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, header=None, names=column_names, delimiter='\\x01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_tokens'] = df['text_tokens'].str.split('\\t')\n",
    "\n",
    "def to_hex_list(x):\n",
    "    output = str(x).split('\\t')\n",
    "#     output = [int(val, 16) for val in str(x).split('\\t')] \n",
    "    return output\n",
    "\n",
    "cols_to_process = ['hashtags', 'present_media', 'present_links', 'present_domains']\n",
    "\n",
    "for col in cols_to_process:  \n",
    "    df[col] = df[col].apply(lambda x: to_hex_list(x) if isinstance(x, str)  else x)\n",
    "\n",
    "    \n",
    "    \n",
    "cols_to_process = ['tweet_timestamp', 'engaging_user_account_creation', 'reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']\n",
    "\n",
    "for col in cols_to_process:  \n",
    "    df[col] = df[col].apply(lambda x: pd.Timestamp(x, unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieIds = df.tweet_id.unique()\n",
    "movieIds.sort()\n",
    "userIds = df.engaging_user_id.append(df.engaged_with_user_id).unique()\n",
    "userIds.sort()\n",
    "\n",
    "m = userIds.size\n",
    "n = movieIds.size\n",
    "\n",
    "movieId_to_movieIDX = dict(zip(movieIds, range(0, movieIds.size)))\n",
    "movieIDX_to_movieId = dict(zip(range(0, movieIds.size), movieIds))\n",
    "\n",
    "userId_to_userIDX = dict(zip(userIds, range(0, userIds.size )))\n",
    "userIDX_to_userId = dict(zip(range(0, userIds.size), userIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"react\"] = df['reply_timestamp'].notnull() | df['retweet_timestamp'].notnull() | df['retweet_with_comment_timestamp'].notnull() | df['like_timestamp'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.concat([df['engaging_user_id'].apply(hash)%10000,\n",
    "                  df['tweet_id'].apply(hash)%10000,\n",
    "                  df['reply_timestamp'].notnull(),\n",
    "                  df['retweet_timestamp'].notnull(),\n",
    "                  df['retweet_with_comment_timestamp'].notnull(),\n",
    "                  df['like_timestamp'].notnull(),df[\"react\"]], axis = 1)\n",
    "\n",
    "ratings.sort_values('engaging_user_id', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_reply = sp.csr_matrix((ratings.reply_timestamp, (ratings.engaging_user_id, ratings.tweet_id)))\n",
    "\n",
    "R_retweet = sp.csr_matrix((ratings.retweet_timestamp, (ratings.engaging_user_id, ratings.tweet_id)))\n",
    "\n",
    "R_retweetwc = sp.csr_matrix((ratings.retweet_with_comment_timestamp, (ratings.engaging_user_id, ratings.tweet_id)))\n",
    "\n",
    "R_like = sp.csr_matrix((ratings.like_timestamp, (ratings.engaging_user_id, ratings.tweet_id)))\n",
    "\n",
    "R_react = sp.csr_matrix((ratings.react, (ratings.engaging_user_id, ratings.tweet_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = R_reply.shape[0]\n",
    "n = R_reply.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_similarities(R,u_id):\n",
    "    uU = np.empty((m,))\n",
    "    \n",
    "    # The loop - left for documentation purposes:\n",
    "    #for i in range(0,R.shape[0]-1):\n",
    "    #    uU[i] = compute_pairwise_user_similarity(u_id, i)\n",
    "    \n",
    "    # generate an copy of initial sparse matrix\n",
    "    R_copy = R.copy()\n",
    "    \n",
    "    # center the data and normalize it afterwards\n",
    "    #R_copy.data = R.data - np.repeat(user_avgs,user_cnts)\n",
    "    R_copy = pp.normalize(R_copy, axis=1)\n",
    "    \n",
    "    # make a copy of the user we want to compute the similarities\n",
    "    u = R_copy[u_id,:].copy()\n",
    "\n",
    "    #When the dot-product was empty this resulted in an array that was not equal to user size\n",
    "    #uU = R_copy.dot(u.T).data\n",
    "    \n",
    "    # Use a combination of toarray() and flatten() instead:\n",
    "    uU = R_copy.dot(u.T).toarray().flatten()\n",
    "    \n",
    "    return uU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default values\n",
    "k = 5\n",
    "with_abs_sim = False\n",
    "\n",
    "def create_user_neighborhood(R,u_id, i_id):\n",
    "    nh = {} ## the neighborhood dict with (user id: similarity) entries\n",
    "    ## nh should not contain u_id and only include users that have rated i_id; there should be at most k neighbors\n",
    "    uU = compute_user_similarities(R,u_id)\n",
    "    uU_copy = uU.copy() ## so that we can modify it, but also keep the original\n",
    "    \n",
    "    user_sums = R.sum(axis=1).A1 ## matrix converted to 1-D array via .A1\n",
    "    user_cnts = (R != 0).sum(axis=1).A1\n",
    "    user_avgs = user_sums / (user_cnts+1) # devide through total number of tweets to get percentage of interaction\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    if with_abs_sim:\n",
    "        uU_copy = np.absolute(uU_copy)\n",
    "    \n",
    "    #straightup exlude the the case where idx=u_id as it will be the first in the array (=1)\n",
    "    uU_idx = np.argsort(uU_copy)[::-1][1:]\n",
    "    \n",
    "    R_dok = R.todok()\n",
    "    i = 0\n",
    "    while len(nh) < k:\n",
    "        # Delete the max element of the similarities (first case = 1)\n",
    "        # Get the ID of the the newest max + the similarity and assign to the intermediate list\n",
    "        if (uU_idx[i],i_id) in R_dok:\n",
    "            nh[uU_idx[i]] = uU[uU_idx[i]]\n",
    "        i = i+1\n",
    "        # failsafe for while loop\n",
    "        if i == len(uU_idx):\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(R, u_id, i_id):\n",
    "    R_dok = R.todok()\n",
    "    if (u_id, i_id) in R_dok:\n",
    "        print(\"user\", u_id, \"has rated item\", i_id, \"with\", R[u_id, i_id])\n",
    "    else:\n",
    "        print(\"user\", u_id, \"has not rated item\", i_id)\n",
    "    \n",
    "    \n",
    "    user_sums = R.sum(axis=1).A1 ## matrix converted to 1-D array via .A1\n",
    "    user_cnts = (R != 0).sum(axis=1).A1\n",
    "    user_avgs = user_sums / (user_cnts+1) # devide through total number of tweets to get percentage of interaction\n",
    "    \n",
    "    nh = create_user_neighborhood(R,u_id, i_id)\n",
    "    \n",
    "    neighborhood_weighted_avg = 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    similarity_sum = 0.5\n",
    "\n",
    "    for key in nh:\n",
    "        neighborhood_weighted_avg = neighborhood_weighted_avg + nh[key]*R[key,i_id]\n",
    "        similarity_sum = similarity_sum + abs(nh[key])\n",
    "    \n",
    "    neighborhood_weighted_avg = neighborhood_weighted_avg/similarity_sum\n",
    "    prediction = neighborhood_weighted_avg\n",
    "    print(f'prediction {prediction:.4f} (user_avg {user_avgs[u_id]:.4f})')\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 0 has not rated item 6800\n",
      "prediction 0.0000 (user_avg 0.5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "predict_rating(R_react,0, 6800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
